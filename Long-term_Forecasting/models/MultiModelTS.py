import numpy as np
import torch
import torch.nn as nn
from torch import optim

# ä½¿ç”¨ Hugging Face çš„ç»Ÿä¸€æ¥å£
from transformers import AutoModel, AutoConfig
from transformers import BertTokenizer, BertModel
from einops import rearrange
from embed import DataEmbedding, DataEmbedding_wo_time


class MultiModelTS(nn.Module):
    """MultiModelTS: æ”¯æŒå¤šç§é¢„è®­ç»ƒæ¨¡å‹çš„æ—¶é—´åºåˆ—é¢„æµ‹æ¶æ„
    
    è¿™æ˜¯ä¸€ä¸ªçµæ´»çš„æ—¶é—´åºåˆ—é¢„æµ‹æ¡†æ¶ï¼Œä½¿ç”¨ Hugging Face çš„ç»Ÿä¸€æ¥å£æ”¯æŒå„ç§é¢„è®­ç»ƒæ¨¡å‹ã€‚
    é€šè¿‡å°†æ—¶é—´åºåˆ—æ•°æ®è½¬æ¢ä¸º patch è¡¨ç¤ºï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å¼ºå¤§çš„è¡¨å¾å­¦ä¹ èƒ½åŠ›æ¥è¿›è¡Œé•¿æœŸæ—¶é—´åºåˆ—é¢„æµ‹ã€‚
    
    ç†è®ºä¾æ®:
        - åŸºäº "Attention Is All You Need" çš„ Transformer æ¶æ„
        - å€Ÿé‰´ Vision Transformer (ViT) çš„ patch åˆ†å‰²ç­–ç•¥  
        - åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„é¢†åŸŸçŸ¥è¯†è¿ç§»èƒ½åŠ›
        - å®ç°è·¨æ¨¡æ€çŸ¥è¯†è¿ç§»ï¼šä»è¯­è¨€å»ºæ¨¡åˆ°æ—¶é—´åºåˆ—é¢„æµ‹
    
    æ”¯æŒçš„æ¨¡å‹ç±»å‹:
        - ä»»ä½•åŸºäº Transformer çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆé€šè¿‡ AutoModel æ¥å£ï¼‰
        - æœ¬åœ°æ¨¡å‹è·¯å¾„æˆ– Hugging Face Hub æ¨¡å‹åç§°
        - GPT-2, BERT, LLaMA, Qwen, RoBERTa, DistilBERT ç­‰
    
    ä¸»è¦ç»„ä»¶:
        - Patch åˆ†å‰²å±‚: å°†æ—¶é—´åºåˆ—åˆ‡åˆ†ä¸ºå›ºå®šå¤§å°çš„ patch
        - çº¿æ€§æŠ•å½±å±‚: å°† patch æ˜ å°„åˆ°æ¨¡å‹çš„éšè—ç»´åº¦
        - é¢„è®­ç»ƒæ¨¡å‹ç¼–ç å™¨: æä¾›å¼ºå¤§çš„åºåˆ—å»ºæ¨¡èƒ½åŠ›
        - è¾“å‡ºæŠ•å½±å±‚: å°†ç¼–ç åçš„ç‰¹å¾æ˜ å°„å›é¢„æµ‹é•¿åº¦
    
    è¾“å…¥å¼ é‡å½¢çŠ¶:
        x: (batch_size, seq_len, num_variables) - è¾“å…¥çš„å¤šå˜é‡æ—¶é—´åºåˆ—
    
    è¾“å‡ºå¼ é‡å½¢çŠ¶:
        outputs: (batch_size, pred_len, num_variables) - é¢„æµ‹çš„å¤šå˜é‡æ—¶é—´åºåˆ—
    
    ä½¿ç”¨ç¤ºä¾‹:
        ```python
        import torch
        from types import SimpleNamespace
        
        # é…ç½®å‚æ•° - ä½¿ç”¨æœ¬åœ°æ¨¡å‹
        configs = SimpleNamespace(
            model_name_or_path='/path/to/local/model',  # æœ¬åœ°æ¨¡å‹è·¯å¾„
            # æˆ–è€…ä½¿ç”¨ Hub æ¨¡å‹åç§°
            # model_name_or_path='bert-base-uncased',
            patch_size=16,
            stride=8,
            seq_len=336,
            pred_len=96,
            d_model=768,
            model_layers=6,
            freeze=True,
            trust_remote_code=True  # å¯¹äºæŸäº›æ¨¡å‹å¯èƒ½éœ€è¦
        )
        
        # åˆ›å»ºæ¨¡å‹
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = MultiModelTS(configs, device)
        
        # å‡†å¤‡è¾“å…¥æ•°æ®
        batch_size, seq_len, num_variables = 32, 336, 7
        x = torch.randn(batch_size, seq_len, num_variables).to(device)
        
        # å‰å‘ä¼ æ’­
        predictions = model(x, itr=0)  # è¾“å‡ºå½¢çŠ¶: (32, 96, 7)
        ```
    """
    
    def __init__(self, configs, device):
        """åˆå§‹åŒ– MultiModelTS æ¨¡å‹
        
        Args:
            configs: æ¨¡å‹é…ç½®å¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å±æ€§:
                model_name_or_path (str): æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„
                patch_size (int): æ¯ä¸ª patch çš„å¤§å°ï¼ˆæ—¶é—´æ­¥æ•°ï¼‰
                stride (int): patch åˆ†å‰²æ—¶çš„æ­¥é•¿
                seq_len (int): è¾“å…¥åºåˆ—é•¿åº¦
                pred_len (int): é¢„æµ‹é•¿åº¦
                d_model (int): æ¨¡å‹çš„éšè—ç»´åº¦
                model_layers (int): ä½¿ç”¨çš„æ¨¡å‹å±‚æ•°
                freeze (bool): æ˜¯å¦å†»ç»“æ¨¡å‹çš„éƒ¨åˆ†å‚æ•°
                trust_remote_code (bool, optional): æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
                use_auth_token (bool/str, optional): Hugging Face è®¤è¯ä»¤ç‰Œ
            device (torch.device): è¿è¡Œè®¾å¤‡ (CPU/GPU)
        """
        super(MultiModelTS, self).__init__()
        
        # (1) ä¿å­˜æ ¸å¿ƒé…ç½®å‚æ•°
        self.model_name_or_path = configs.model_name_or_path
        self.patch_size = configs.patch_size
        self.stride = configs.stride
        
        # (2) è®¡ç®— patch æ•°é‡ï¼šä½¿ç”¨æ»‘åŠ¨çª—å£æ–¹å¼åˆ†å‰²æ—¶é—´åºåˆ—
        # å…¬å¼: (åºåˆ—é•¿åº¦ - patchå¤§å°) // æ­¥é•¿ + 1
        self.patch_num = (configs.seq_len - self.patch_size) // self.stride + 1

        # (3) åˆ›å»ºå¡«å……å±‚ï¼Œç¡®ä¿æœ€åä¸€ä¸ª patch èƒ½å¤Ÿå®Œæ•´æå–
        # ReplicationPad1d(0, stride) åœ¨åºåˆ—æœ«å°¾å¤åˆ¶æœ€å stride ä¸ªå…ƒç´ 
        self.padding_patch_layer = nn.ReplicationPad1d((0, self.stride)) 
        self.patch_num += 1  # å› ä¸ºå¡«å……åä¼šå¤šå‡ºä¸€ä¸ª patch
        
        # (4) åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„ AutoModel æ¥å£ï¼‰
        self.model = self._load_pretrained_model(configs)
        
        # (5) è·å–æ¨¡å‹çš„å®é™…éšè—ç»´åº¦
        self.d_model = self._get_model_hidden_size()
        
        # (6) é™åˆ¶ä½¿ç”¨çš„å±‚æ•°ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if hasattr(configs, 'model_layers') and configs.model_layers > 0:
            self._limit_model_layers(configs.model_layers)
        
        # (7) åˆ›å»ºè¾“å…¥æŠ•å½±å±‚ï¼špatch_size -> d_model
        # å°†æ¯ä¸ª patch æ˜ å°„åˆ°æ¨¡å‹çš„éšè—ç»´åº¦ç©ºé—´
        self.in_layer = nn.Linear(configs.patch_size, self.d_model)
        
        # (8) åˆ›å»ºè¾“å‡ºæŠ•å½±å±‚ï¼š(d_model * patch_num) -> pred_len
        # å°†æ‰€æœ‰ patch çš„ç¼–ç ç‰¹å¾æ‹¼æ¥åæ˜ å°„åˆ°é¢„æµ‹é•¿åº¦
        self.out_layer = nn.Linear(self.d_model * self.patch_num, configs.pred_len)
        
        # (9) å‚æ•°å†»ç»“ç­–ç•¥
        if hasattr(configs, 'freeze') and configs.freeze:
            self._apply_freeze_strategy()

        # (10) å°†æ‰€æœ‰æ¨¡å—ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡å¹¶è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        for layer in (self.model, self.in_layer, self.out_layer):
            layer.to(device=device)
            layer.train()
        
        self.cnt = 0

    def _load_pretrained_model(self, configs):
        """ä½¿ç”¨ AutoModel ç»Ÿä¸€åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        
        Args:
            configs: é…ç½®å¯¹è±¡
            
        Returns:
            torch.nn.Module: åŠ è½½çš„é¢„è®­ç»ƒæ¨¡å‹
        """
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {configs.model_name_or_path}")
        
        try:
            # å‡†å¤‡åŠ è½½å‚æ•°
            load_kwargs = {
                'output_attentions': True,
                'output_hidden_states': True,
            }
            
            # æ·»åŠ å¯é€‰å‚æ•°
            if hasattr(configs, 'trust_remote_code') and configs.trust_remote_code:
                load_kwargs['trust_remote_code'] = True
                
            if hasattr(configs, 'use_auth_token') and configs.use_auth_token:
                load_kwargs['use_auth_token'] = configs.use_auth_token
            
            # å°è¯•åŠ è½½æ¨¡å‹
            model = AutoModel.from_pretrained(
                configs.model_name_or_path,
                **load_kwargs
            )
            
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {configs.model_name_or_path}")
            print(f"ğŸ“Š æ¨¡å‹ç±»å‹: {model.__class__.__name__}")
            
            return model
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {configs.model_name_or_path}")
            print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
            print("ğŸ’¡ è¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–è€…æ˜¯å¦éœ€è¦è®¾ç½® trust_remote_code=True")
            raise e

    def _get_model_hidden_size(self):
        """è·å–æ¨¡å‹çš„éšè—ç»´åº¦å¤§å°
        
        Returns:
            int: éšè—ç»´åº¦å¤§å°
        """
        # å°è¯•ä»é…ç½®ä¸­è·å–éšè—ç»´åº¦
        if hasattr(self.model, 'config'):
            config = self.model.config
            
            # å¸¸è§çš„éšè—ç»´åº¦å±æ€§å
            hidden_size_attrs = ['hidden_size', 'd_model', 'n_embd', 'dim', 'model_dim']
            
            for attr in hidden_size_attrs:
                if hasattr(config, attr):
                    hidden_size = getattr(config, attr)
                    print(f"ğŸ“ æ£€æµ‹åˆ°æ¨¡å‹éšè—ç»´åº¦: {hidden_size} (æ¥è‡ª config.{attr})")
                    return hidden_size
        
        # å¦‚æœæ— æ³•ä»é…ç½®è·å–ï¼Œå°è¯•æ¨æ–­
        print("âš ï¸  æ— æ³•ä»é…ç½®è·å–éšè—ç»´åº¦ï¼Œå°è¯•æ¨æ–­...")
        
        # åˆ›å»ºä¸€ä¸ªå°çš„æµ‹è¯•è¾“å…¥æ¥æ¨æ–­ç»´åº¦
        test_input = torch.randn(1, 8, 768)  # å‡è®¾çš„è¾“å…¥
        
        try:
            with torch.no_grad():
                output = self.model(inputs_embeds=test_input)
                if hasattr(output, 'last_hidden_state'):
                    hidden_size = output.last_hidden_state.shape[-1]
                    print(f"ğŸ“ é€šè¿‡æ¨æ–­å¾—åˆ°éšè—ç»´åº¦: {hidden_size}")
                    return hidden_size
        except:
            pass
        
        # é»˜è®¤å€¼
        default_size = 768
        print(f"âš ï¸  æ— æ³•ç¡®å®šéšè—ç»´åº¦ï¼Œä½¿ç”¨é»˜è®¤å€¼: {default_size}")
        return default_size

    def _limit_model_layers(self, max_layers):
        """é™åˆ¶æ¨¡å‹ä½¿ç”¨çš„å±‚æ•°
        
        Args:
            max_layers (int): æœ€å¤§å±‚æ•°
        """
        print(f"ğŸ”§ é™åˆ¶æ¨¡å‹å±‚æ•°ä¸º: {max_layers}")
        
        # å°è¯•ä¸åŒçš„å±‚å±æ€§åç§°
        layer_attrs = ['layers', 'layer', 'h', 'encoder.layer', 'transformer.h']
        
        for attr_path in layer_attrs:
            try:
                # æ”¯æŒåµŒå¥—å±æ€§
                obj = self.model
                for attr in attr_path.split('.'):
                    obj = getattr(obj, attr)
                
                if hasattr(obj, '__len__') and len(obj) > max_layers:
                    # æˆªæ–­å±‚æ•°
                    if hasattr(obj, '__setitem__'):
                        # å¯¹äº ModuleList
                        new_layers = obj[:max_layers]
                        obj.clear()
                        obj.extend(new_layers)
                    else:
                        # å¯¹äºå…¶ä»–ç±»å‹ï¼Œå°è¯•ç›´æ¥èµ‹å€¼
                        setattr(obj, attr_path.split('.')[-1], obj[:max_layers])
                    
                    print(f"âœ… æˆåŠŸé™åˆ¶ {attr_path} å±‚æ•°: {len(obj)} -> {max_layers}")
                    return
                    
            except (AttributeError, TypeError):
                continue
        
        print("âš ï¸  æ— æ³•é™åˆ¶æ¨¡å‹å±‚æ•°ï¼Œå°†ä½¿ç”¨å®Œæ•´æ¨¡å‹")

    def _apply_freeze_strategy(self):
        """åº”ç”¨é€šç”¨çš„å‚æ•°å†»ç»“ç­–ç•¥
        
        å†»ç»“å¤§éƒ¨åˆ†å‚æ•°ï¼Œåªè®­ç»ƒå½’ä¸€åŒ–å±‚å’ŒåµŒå…¥å±‚ç›¸å…³çš„å‚æ•°
        """
        print("ğŸ§Š åº”ç”¨å‚æ•°å†»ç»“ç­–ç•¥...")
        
        total_params = 0
        frozen_params = 0
        trainable_params = 0
        
        for name, param in self.model.named_parameters():
            total_params += param.numel()
            
            # å®šä¹‰éœ€è¦è®­ç»ƒçš„å‚æ•°æ¨¡å¼
            trainable_patterns = [
                'norm',           # å„ç§å½’ä¸€åŒ–å±‚
                'ln',             # LayerNorm (GPT é£æ ¼)
                'layer_norm',     # LayerNorm (BERT é£æ ¼)
                'rmsnorm',        # RMSNorm (LLaMA é£æ ¼)
                'embed',          # åµŒå…¥å±‚
                'position',       # ä½ç½®ç¼–ç 
                'wpe',            # ä½ç½®åµŒå…¥ (GPT é£æ ¼)
                'word_embed',     # è¯åµŒå…¥
                'token_embed',    # TokenåµŒå…¥
            ]
            
            # æ£€æŸ¥å‚æ•°åæ˜¯å¦åŒ¹é…å¯è®­ç»ƒæ¨¡å¼
            should_train = any(pattern in name.lower() for pattern in trainable_patterns)
            
            if should_train:
                param.requires_grad = True
                trainable_params += param.numel()
            else:
                param.requires_grad = False
                frozen_params += param.numel()
        
        freeze_ratio = frozen_params / total_params * 100
        print(f"ğŸ“Š å‚æ•°å†»ç»“ç»Ÿè®¡:")
        print(f"   - æ€»å‚æ•°æ•°: {total_params:,}")
        print(f"   - å†»ç»“å‚æ•°: {frozen_params:,}")
        print(f"   - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"   - å†»ç»“æ¯”ä¾‹: {freeze_ratio:.1f}%")

    def forward(self, x, itr):
        """MultiModelTS æ¨¡å‹çš„å‰å‘ä¼ æ’­è¿‡ç¨‹
        
        å®ç°å®Œæ•´çš„æ—¶é—´åºåˆ—é¢„æµ‹æµç¨‹ï¼Œæ”¯æŒä»»ä½•åŸºäº Transformer çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚
        
        Args:
            x (torch.Tensor): è¾“å…¥çš„å¤šå˜é‡æ—¶é—´åºåˆ—
                å½¢çŠ¶: (batch_size, seq_len, num_variables)
            itr (int): å½“å‰è¿­ä»£æ¬¡æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼Œæ¨¡å‹å†…éƒ¨æœªä½¿ç”¨ï¼‰
        
        Returns:
            torch.Tensor: é¢„æµ‹çš„å¤šå˜é‡æ—¶é—´åºåˆ—
                å½¢çŠ¶: (batch_size, pred_len, num_variables)
        
        ç®—æ³•æµç¨‹è¯¦è§£:
            1. æ•°æ®æ ‡å‡†åŒ–ï¼ˆZ-scoreï¼‰
            2. ç»´åº¦é‡æ’åˆ—ï¼Œä¾¿äº patch æ“ä½œ
            3. åºåˆ—å¡«å……å’Œ patch åˆ†å‰²
            4. é¢„è®­ç»ƒæ¨¡å‹ç¼–ç ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
            5. é¢„æµ‹ç”Ÿæˆå’Œåæ ‡å‡†åŒ–
        """
        # è·å–è¾“å…¥å¼ é‡çš„ç»´åº¦ä¿¡æ¯
        B, L, M = x.shape  # B=batch_size, L=seq_len, M=num_variables

        # ==================== (1) æ•°æ®æ ‡å‡†åŒ– ====================
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬åœ¨æ—¶é—´ç»´åº¦ä¸Šçš„å‡å€¼ï¼Œç”¨äºä¸­å¿ƒåŒ–
        # (B, L, M) -> (B, 1, M)
        means = x.mean(1, keepdim=True).detach()
        x = x - means  # é›¶å‡å€¼åŒ–
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬åœ¨æ—¶é—´ç»´åº¦ä¸Šçš„æ ‡å‡†å·®ï¼Œç”¨äºæ ‡å‡†åŒ–
        # (B, L, M) -> (B, 1, M)
        stdev = torch.sqrt(torch.var(x, dim=1, keepdim=True, unbiased=False)+ 1e-5).detach() 
        x /= stdev  # æ ‡å‡†åŒ–åˆ°å•ä½æ–¹å·®

        # ==================== (2) ç»´åº¦é‡æ’åˆ— ====================
        # å°†æ—¶é—´ç»´åº¦ç§»åˆ°æœ€åï¼Œä¾¿äºåç»­çš„ patch æ“ä½œ
        # (B, L, M) -> (B, M, L)
        x = rearrange(x, 'b l m -> b m l')

        # ==================== (3) Patch åˆ†å‰² ====================
        # åœ¨åºåˆ—æœ«å°¾å¡«å……ï¼Œç¡®ä¿èƒ½æå–å®Œæ•´çš„ patch
        # (B, M, L) -> (B, M, L + stride)
        x = self.padding_patch_layer(x)
        
        # ä½¿ç”¨æ»‘åŠ¨çª—å£æå– patch
        # unfold(dimension=-1, size=patch_size, step=stride)
        # (B, M, L + stride) -> (B, M, patch_num, patch_size)
        x = x.unfold(dimension=-1, size=self.patch_size, step=self.stride)
        
        # é‡æ–°æ’åˆ—ç»´åº¦ï¼Œå°† batch å’Œå˜é‡ç»´åº¦åˆå¹¶
        # (B, M, patch_num, patch_size) -> (B*M, patch_num, patch_size)
        x = rearrange(x, 'b m n p -> (b m) n p')

        # ==================== (4) çº¿æ€§æŠ•å½±åˆ°æ¨¡å‹éšè—ç©ºé—´ ====================
        # å°†æ¯ä¸ª patch æ˜ å°„åˆ° d_model ç»´åº¦
        # (B*M, patch_num, patch_size) -> (B*M, patch_num, d_model)
        outputs = self.in_layer(x)
        
        # ==================== (5) é¢„è®­ç»ƒæ¨¡å‹ç¼–ç  ====================
        outputs = self._encode_with_model(outputs)

        # ==================== (6) é¢„æµ‹ç”Ÿæˆ ====================
        # å°†æ‰€æœ‰ patch çš„ç‰¹å¾å±•å¹³åè¿›è¡Œæœ€ç»ˆé¢„æµ‹
        # (B*M, patch_num, d_model) -> (B*M, patch_num * d_model) -> (B*M, pred_len)
        outputs = self.out_layer(outputs.reshape(B*M, -1))
        
        # æ¢å¤åŸå§‹çš„ batch å’Œå˜é‡ç»´åº¦
        # (B*M, pred_len) -> (B, pred_len, M)
        outputs = rearrange(outputs, '(b m) l -> b l m', b=B)

        # ==================== (7) åæ ‡å‡†åŒ– ====================
        # æ¢å¤åŸå§‹çš„å°ºåº¦å’Œå‡å€¼
        outputs = outputs * stdev  # æ¢å¤æ ‡å‡†å·®
        outputs = outputs + means  # æ¢å¤å‡å€¼

        return outputs
    
    def _encode_with_model(self, x):
        """ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œç¼–ç ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
        
        Args:
            x (torch.Tensor): è¾“å…¥çš„ patch åµŒå…¥
                å½¢çŠ¶: (B*M, patch_num, d_model)
        
        Returns:
            torch.Tensor: ç¼–ç åçš„ç‰¹å¾
                å½¢çŠ¶: (B*M, patch_num, d_model)
        """
        try:
            # ä½¿ç”¨ inputs_embeds å‚æ•°è¿›è¡Œç¼–ç 
            outputs = self.model(inputs_embeds=x)
            
            # å°è¯•è·å–æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
            if hasattr(outputs, 'last_hidden_state'):
                return outputs.last_hidden_state
            elif hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
                return outputs.hidden_states[-1]  # å–æœ€åä¸€å±‚
            elif isinstance(outputs, tuple) and len(outputs) > 0:
                return outputs[0]  # é€šå¸¸ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ä¸»è¦è¾“å‡º
            else:
                # å¦‚æœéƒ½ä¸è¡Œï¼Œç›´æ¥è¿”å› outputs
                return outputs
                
        except Exception as e:
            print(f"âš ï¸  æ¨¡å‹ç¼–ç è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            print("ğŸ’¡ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•...")
            
            # å¤‡ç”¨æ–¹æ³•ï¼šå¦‚æœæ¨¡å‹ä¸æ”¯æŒ inputs_embedsï¼Œå°è¯•å…¶ä»–æ–¹å¼
            try:
                # æœ‰äº›æ¨¡å‹å¯èƒ½éœ€è¦ä¸åŒçš„è¾“å…¥å‚æ•°
                outputs = self.model(hidden_states=x)
                if hasattr(outputs, 'last_hidden_state'):
                    return outputs.last_hidden_state
                return outputs[0] if isinstance(outputs, tuple) else outputs
            except:
                # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥è¿”å›è¾“å…¥ï¼ˆç›¸å½“äºè·³è¿‡ç¼–ç ï¼‰
                print("âŒ æ— æ³•ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ç¼–ç ï¼Œå°†è·³è¿‡ç¼–ç æ­¥éª¤")
                return x


# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸå§‹ç±»åä½œä¸ºåˆ«å
GPT4TS = MultiModelTS
