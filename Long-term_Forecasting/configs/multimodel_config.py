#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¤šæ¨¡å‹æ—¶é—´åºåˆ—é¢„æµ‹é…ç½®æ–‡ä»¶
ä½¿ç”¨ Hugging Face ç»Ÿä¸€æ¥å£ï¼Œæ”¯æŒæœ¬åœ°æ¨¡å‹è·¯å¾„å’Œ Hub æ¨¡å‹åç§°
"""

from types import SimpleNamespace
import os

# ============= åŸºç¡€é…ç½®æ¨¡æ¿ =============
BASE_CONFIG = {
    # æ•°æ®ç›¸å…³é…ç½®
    'seq_len': 336,          # è¾“å…¥åºåˆ—é•¿åº¦
    'pred_len': 96,          # é¢„æµ‹åºåˆ—é•¿åº¦
    'patch_size': 16,        # æ¯ä¸ªpatchçš„å¤§å°
    'stride': 8,             # patchçš„æ­¥é•¿
    
    # è®­ç»ƒç›¸å…³é…ç½®
    'batch_size': 32,        # æ‰¹æ¬¡å¤§å°
    'learning_rate': 1e-4,   # å­¦ä¹ ç‡
    'num_epochs': 10,        # è®­ç»ƒè½®æ•°
    'patience': 3,           # æ—©åœè€å¿ƒå€¼
    
    # æ¨¡å‹ç›¸å…³é…ç½®
    'freeze': True,          # æ˜¯å¦å†»ç»“éƒ¨åˆ†å‚æ•°
    'dropout': 0.1,          # Dropoutç‡
    'trust_remote_code': False,  # æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
    
    # å…¶ä»–é…ç½®
    'device': 'auto',        # è®¾å¤‡é€‰æ‹© ('auto', 'cpu', 'cuda')
    'seed': 42,              # éšæœºç§å­
}


# ============= é¢„è®¾æ¨¡å‹é…ç½® =============

# GPT-2 ç³»åˆ—æ¨¡å‹é…ç½®
GPT2_MODELS = {
    'gpt2': {
        'model_name_or_path': 'gpt2',
        'description': 'GPT-2 base (124M å‚æ•°)',
        'd_model': 768,
        'model_layers': 6,
        'learning_rate': 1e-4,
        'batch_size': 32,
    },
    'gpt2-medium': {
        'model_name_or_path': 'gpt2-medium',  
        'description': 'GPT-2 medium (355M å‚æ•°)',
        'd_model': 1024,
        'model_layers': 4,
        'learning_rate': 5e-5,
        'batch_size': 16,
    },
    'gpt2-large': {
        'model_name_or_path': 'gpt2-large',
        'description': 'GPT-2 large (774M å‚æ•°)',
        'd_model': 1280,
        'model_layers': 3,
        'learning_rate': 2e-5,
        'batch_size': 8,
    },
}

# BERT ç³»åˆ—æ¨¡å‹é…ç½®
BERT_MODELS = {
    'bert-base': {
        'model_name_or_path': 'bert-base-uncased',
        'description': 'BERT base uncased (110M å‚æ•°)',
        'd_model': 768,
        'model_layers': 6,
        'learning_rate': 2e-5,
        'batch_size': 16,
    },
    'bert-large': {
        'model_name_or_path': 'bert-large-uncased',
        'description': 'BERT large uncased (340M å‚æ•°)',
        'd_model': 1024,
        'model_layers': 4,
        'learning_rate': 1e-5,
        'batch_size': 8,
    },
    'distilbert': {
        'model_name_or_path': 'distilbert-base-uncased',
        'description': 'DistilBERT base (66M å‚æ•°)',
        'd_model': 768,
        'model_layers': 6,
        'learning_rate': 3e-5,
        'batch_size': 32,
    },
    'roberta-base': {
        'model_name_or_path': 'roberta-base',
        'description': 'RoBERTa base (125M å‚æ•°)',
        'd_model': 768,
        'model_layers': 6,
        'learning_rate': 2e-5,
        'batch_size': 16,
    },
}

# å¤§è¯­è¨€æ¨¡å‹é…ç½®
LLM_MODELS = {
    'llama2-7b': {
        'model_name_or_path': 'meta-llama/Llama-2-7b-hf',
        'description': 'LLaMA 2 7B (7B å‚æ•°)',
        'd_model': 4096,
        'model_layers': 4,
        'learning_rate': 1e-5,
        'batch_size': 4,
        'trust_remote_code': True,
    },
    'qwen-7b': {
        'model_name_or_path': 'Qwen/Qwen-7B',
        'description': 'Qwen 7B (7B å‚æ•°)',
        'd_model': 4096,
        'model_layers': 4,
        'learning_rate': 1e-5,
        'batch_size': 4,
        'trust_remote_code': True,
    },
    'qwen-1.8b': {
        'model_name_or_path': 'Qwen/Qwen-1_8B',
        'description': 'Qwen 1.8B (1.8B å‚æ•°)',
        'd_model': 2048,
        'model_layers': 6,
        'learning_rate': 5e-5,
        'batch_size': 8,
        'trust_remote_code': True,
    },
}

# åˆå¹¶æ‰€æœ‰é¢„è®¾æ¨¡å‹
PRESET_MODELS = {
    **GPT2_MODELS,
    **BERT_MODELS, 
    **LLM_MODELS
}


# ============= å®éªŒé…ç½®ç»„åˆ =============
EXPERIMENT_CONFIGS = {
    'quick_test': {
        # å¿«é€Ÿæµ‹è¯•é…ç½®ï¼ˆé€‚ç”¨äºæ‰€æœ‰æ¨¡å‹ï¼‰
        'seq_len': 168,      # è¾ƒçŸ­çš„åºåˆ—
        'pred_len': 24,      # è¾ƒçŸ­çš„é¢„æµ‹
        'num_epochs': 2,     # å°‘é‡è½®æ•°
        'batch_size': 4,     # å°æ‰¹æ¬¡
        'model_layers': 2,   # å°‘é‡å±‚æ•°
    },
    
    'small_scale': {
        # å°è§„æ¨¡å®éªŒé…ç½®
        'seq_len': 336,
        'pred_len': 96,
        'num_epochs': 5,
        'batch_size': 8,
        'model_layers': 4,
    },
    
    'full_scale': {
        # å®Œæ•´è§„æ¨¡å®éªŒé…ç½®
        'seq_len': 720,      # æ›´é•¿çš„åºåˆ—
        'pred_len': 192,     # æ›´é•¿çš„é¢„æµ‹
        'num_epochs': 20,
        'batch_size': 16,
        'model_layers': 8,
    }
}


def get_model_config(model_key_or_path, experiment_type='small_scale'):
    """è·å–æŒ‡å®šæ¨¡å‹å’Œå®éªŒç±»å‹çš„é…ç½®
    
    Args:
        model_key_or_path (str): é¢„è®¾æ¨¡å‹é”®åæˆ–æ¨¡å‹è·¯å¾„
            - é¢„è®¾æ¨¡å‹: 'gpt2', 'bert-base', 'llama2-7b' ç­‰
            - æœ¬åœ°è·¯å¾„: '/path/to/local/model'
            - Hub åç§°: 'microsoft/DialoGPT-medium'
        experiment_type (str): å®éªŒç±»å‹ ('quick_test', 'small_scale', 'full_scale')
    
    Returns:
        SimpleNamespace: é…ç½®å¯¹è±¡
    
    Examples:
        ```python
        # ä½¿ç”¨é¢„è®¾æ¨¡å‹
        config = get_model_config('gpt2', 'quick_test')
        
        # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
        config = get_model_config('/home/user/my_model', 'small_scale')
        
        # ä½¿ç”¨ Hub æ¨¡å‹åç§°
        config = get_model_config('microsoft/DialoGPT-medium', 'full_scale')
        ```
    """
    # è·å–å®éªŒé…ç½®
    if experiment_type in EXPERIMENT_CONFIGS:
        experiment_config = EXPERIMENT_CONFIGS[experiment_type]
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„å®éªŒç±»å‹: {experiment_type}")
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºé¢„è®¾æ¨¡å‹
    if model_key_or_path in PRESET_MODELS:
        # ä½¿ç”¨é¢„è®¾æ¨¡å‹é…ç½®
        model_config = PRESET_MODELS[model_key_or_path].copy()
        print(f"ğŸ“‹ ä½¿ç”¨é¢„è®¾æ¨¡å‹é…ç½®: {model_key_or_path}")
        print(f"ğŸ“ æ¨¡å‹æè¿°: {model_config.get('description', 'N/A')}")
    else:
        # ä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„æˆ–åç§°
        model_config = {
            'model_name_or_path': model_key_or_path,
            'description': f'è‡ªå®šä¹‰æ¨¡å‹: {model_key_or_path}',
            'd_model': 768,  # é»˜è®¤å€¼ï¼Œä¼šåœ¨è¿è¡Œæ—¶è‡ªåŠ¨æ£€æµ‹
            'model_layers': 6,
            'learning_rate': 1e-4,
            'batch_size': 16,
        }
        print(f"ğŸ“‹ ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹: {model_key_or_path}")
        
        # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨
        if os.path.exists(model_key_or_path):
            print(f"âœ… æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„: {model_key_or_path}")
        else:
            print(f"ğŸŒ å°†å°è¯•ä» Hugging Face Hub åŠ è½½: {model_key_or_path}")
    
    # åˆå¹¶åŸºç¡€é…ç½®ã€æ¨¡å‹é…ç½®å’Œå®éªŒé…ç½®
    final_config = {
        **BASE_CONFIG,
        **model_config,
        **experiment_config
    }
    
    return SimpleNamespace(**final_config)


def get_preset_model_list():
    """è·å–æ‰€æœ‰é¢„è®¾æ¨¡å‹çš„åˆ—è¡¨
    
    Returns:
        dict: æŒ‰ç±»åˆ«ç»„ç»‡çš„æ¨¡å‹åˆ—è¡¨
    """
    return {
        'GPT-2 ç³»åˆ—': {k: v['description'] for k, v in GPT2_MODELS.items()},
        'BERT ç³»åˆ—': {k: v['description'] for k, v in BERT_MODELS.items()},
        'å¤§è¯­è¨€æ¨¡å‹': {k: v['description'] for k, v in LLM_MODELS.items()},
    }


def create_local_model_config(local_path, **kwargs):
    """ä¸ºæœ¬åœ°æ¨¡å‹åˆ›å»ºé…ç½®
    
    Args:
        local_path (str): æœ¬åœ°æ¨¡å‹è·¯å¾„
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
    
    Returns:
        SimpleNamespace: é…ç½®å¯¹è±¡
    """
    if not os.path.exists(local_path):
        raise FileNotFoundError(f"æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {local_path}")
    
    config_dict = {
        **BASE_CONFIG,
        'model_name_or_path': local_path,
        'description': f'æœ¬åœ°æ¨¡å‹: {local_path}',
        'd_model': 768,  # é»˜è®¤å€¼ï¼Œä¼šè‡ªåŠ¨æ£€æµ‹
        'model_layers': 6,
        'learning_rate': 1e-4,
        'batch_size': 16,
        'trust_remote_code': True,  # æœ¬åœ°æ¨¡å‹é€šå¸¸éœ€è¦ä¿¡ä»»ä»£ç 
        **kwargs  # è¦†ç›–é»˜è®¤å‚æ•°
    }
    
    return SimpleNamespace(**config_dict)


def print_config_summary(config):
    """æ‰“å°é…ç½®æ‘˜è¦
    
    Args:
        config (SimpleNamespace): é…ç½®å¯¹è±¡
    """
    print(f"ğŸ“Š æ¨¡å‹é…ç½®æ‘˜è¦:")
    print(f"   æ¨¡å‹è·¯å¾„: {config.model_name_or_path}")
    if hasattr(config, 'description'):
        print(f"   æ¨¡å‹æè¿°: {config.description}")
    print(f"   éšè—ç»´åº¦: {config.d_model}")
    print(f"   æ¨¡å‹å±‚æ•°: {config.model_layers}")
    print(f"   åºåˆ—é•¿åº¦: {config.seq_len}")
    print(f"   é¢„æµ‹é•¿åº¦: {config.pred_len}")
    print(f"   æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"   å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"   è®­ç»ƒè½®æ•°: {config.num_epochs}")
    print(f"   å‚æ•°å†»ç»“: {config.freeze}")
    print(f"   ä¿¡ä»»è¿œç¨‹ä»£ç : {config.trust_remote_code}")


def print_available_models():
    """æ‰“å°æ‰€æœ‰å¯ç”¨çš„é¢„è®¾æ¨¡å‹"""
    print("ğŸ¯ å¯ç”¨çš„é¢„è®¾æ¨¡å‹:")
    print("=" * 60)
    
    model_categories = get_preset_model_list()
    
    for category, models in model_categories.items():
        print(f"\nğŸ“š {category}:")
        for key, description in models.items():
            print(f"   {key:<15} - {description}")
    
    print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"   config = get_model_config('gpt2', 'quick_test')")
    print(f"   config = get_model_config('/path/to/local/model', 'small_scale')")


if __name__ == "__main__":
    print("=== å¤šæ¨¡å‹æ—¶é—´åºåˆ—é¢„æµ‹é…ç½®ç³»ç»Ÿ ===\n")
    
    # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
    print_available_models()
    
    print(f"\n" + "=" * 60)
    print("ğŸ“‹ é…ç½®ç¤ºä¾‹:")
    print("=" * 60)
    
    # æ¼”ç¤ºä¸åŒçš„é…ç½®æ–¹å¼
    examples = [
        ('gpt2', 'quick_test'),
        ('bert-base', 'small_scale'), 
        ('distilbert', 'quick_test'),
    ]
    
    for model_key, exp_type in examples:
        print(f"\n--- {model_key.upper()} ({exp_type}) ---")
        try:
            config = get_model_config(model_key, exp_type)
            print_config_summary(config)
        except Exception as e:
            print(f"âŒ é…ç½®å¤±è´¥: {str(e)}")
    
    print(f"\n" + "=" * 60)
    print("ğŸ”§ æœ¬åœ°æ¨¡å‹é…ç½®ç¤ºä¾‹:")
    print("=" * 60)
    print("# ä¸ºæœ¬åœ°æ¨¡å‹åˆ›å»ºé…ç½®")
    print("config = create_local_model_config(")
    print("    '/path/to/my/model',")
    print("    model_layers=4,")
    print("    batch_size=8")
    print(")")
    print("\n# æˆ–è€…ç›´æ¥ä½¿ç”¨è·¯å¾„")
    print("config = get_model_config('/path/to/my/model', 'small_scale')") 