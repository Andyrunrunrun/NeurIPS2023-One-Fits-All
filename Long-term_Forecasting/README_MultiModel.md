# å¤šæ¨¡å‹æ—¶é—´åºåˆ—é¢„æµ‹æ¶æ„

è¿™æ˜¯ä¸€ä¸ªåŸºäº Hugging Face ç»Ÿä¸€æ¥å£çš„æ—¶é—´åºåˆ—é¢„æµ‹æ¡†æ¶ï¼Œå¯ä»¥çµæ´»åœ°ä½¿ç”¨ä»»ä½•åŸºäº Transformer çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ï¼ŒåŒ…æ‹¬æœ¬åœ°æ¨¡å‹å’Œ Hub æ¨¡å‹ã€‚

## ğŸŒŸ ä¸»è¦ç‰¹æ€§

- **ç»Ÿä¸€æ¥å£**: ä½¿ç”¨ Hugging Face AutoModel ç»Ÿä¸€åŠ è½½æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹
- **æœ¬åœ°æ¨¡å‹æ”¯æŒ**: æ”¯æŒä»æœ¬åœ°è·¯å¾„åŠ è½½è‡ªå®šä¹‰è®­ç»ƒçš„æ¨¡å‹
- **ä¸°å¯Œçš„é¢„è®¾æ¨¡å‹**: å†…ç½® GPT-2ã€BERTã€DistilBERTã€RoBERTa ç­‰å¤šç§æ¨¡å‹é…ç½®
- **çµæ´»é…ç½®**: é€šè¿‡ç®€å•çš„é…ç½®æ–‡ä»¶ç®¡ç†ä¸åŒçš„å®éªŒè®¾ç½®
- **å†…å­˜é«˜æ•ˆ**: æ™ºèƒ½çš„å‚æ•°å†»ç»“ç­–ç•¥ï¼Œå‡å°‘æ˜¾å­˜å ç”¨
- **å®Œæ•´å·¥å…·é“¾**: åŒ…å«è®­ç»ƒã€æµ‹è¯•ã€å¯è§†åŒ–ç­‰å®Œæ•´å·¥å…·

## ğŸ“ æ–‡ä»¶ç»“æ„

```
â”œâ”€â”€ models/
â”‚   â””â”€â”€ GPT4TS.py              # å¤šæ¨¡å‹æ¶æ„å®ç° (MultiModelTS)
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ multimodel_config.py   # é…ç½®ç³»ç»Ÿ
â”œâ”€â”€ test_multimodel.py         # æ¨¡å‹æµ‹è¯•è„šæœ¬
â”œâ”€â”€ train_multimodel.py        # è®­ç»ƒè„šæœ¬
â””â”€â”€ README_MultiModel.md       # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè¦æ±‚

```bash
# åŸºç¡€ä¾èµ–
pip install torch torchvision
pip install transformers>=4.21.0
pip install einops
pip install numpy matplotlib

# å¯é€‰ï¼šç”¨äºåŠ é€Ÿå’Œé‡åŒ–
pip install accelerate
pip install bitsandbytes
```

### 2. åŸºç¡€ä½¿ç”¨

```python
from models.GPT4TS import MultiModelTS
from configs.multimodel_config import get_model_config
import torch

# ä½¿ç”¨é¢„è®¾æ¨¡å‹
config = get_model_config('gpt2', 'quick_test')

# æˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹
# config = get_model_config('/path/to/local/model', 'small_scale')

# æˆ–ä½¿ç”¨ Hub æ¨¡å‹
# config = get_model_config('microsoft/DialoGPT-medium', 'quick_test')

# åˆ›å»ºæ¨¡å‹
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = MultiModelTS(config, device)

# å‡†å¤‡æ•°æ®
batch_size, seq_len, num_variables = 4, 168, 7
x = torch.randn(batch_size, seq_len, num_variables).to(device)

# é¢„æµ‹
predictions = model(x, itr=0)
print(f"é¢„æµ‹ç»“æœå½¢çŠ¶: {predictions.shape}")
```

## ğŸ¯ æ”¯æŒçš„æ¨¡å‹

### é¢„è®¾æ¨¡å‹

| ç±»åˆ«       | æ¨¡å‹é”®å       | æè¿°         | å‚æ•°é‡ | æ¨èç”¨é€”         |
| ---------- | -------------- | ------------ | ------ | ---------------- |
| **GPT-2**  | `gpt2`         | GPT-2 base   | 124M   | é€šç”¨æ—¶é—´åºåˆ—é¢„æµ‹ |
|            | `gpt2-medium`  | GPT-2 medium | 355M   | å¤æ‚æ¨¡å¼è¯†åˆ«     |
|            | `gpt2-large`   | GPT-2 large  | 774M   | é«˜ç²¾åº¦é¢„æµ‹       |
| **BERT**   | `bert-base`    | BERT base    | 110M   | å¤šå˜é‡å…³è”åˆ†æ   |
|            | `bert-large`   | BERT large   | 340M   | æ·±åº¦ç‰¹å¾æå–     |
|            | `distilbert`   | DistilBERT   | 66M    | è½»é‡çº§éƒ¨ç½²       |
|            | `roberta-base` | RoBERTa base | 125M   | é²æ£’æ€§é¢„æµ‹       |
| **å¤§æ¨¡å‹** | `llama2-7b`    | LLaMA 2 7B   | 7B     | å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›   |
|            | `qwen-7b`      | Qwen 7B      | 7B     | å¤šè¯­è¨€æ”¯æŒ       |
|            | `qwen-1.8b`    | Qwen 1.8B    | 1.8B   | ä¸­ç­‰è§„æ¨¡ä»»åŠ¡     |

### è‡ªå®šä¹‰æ¨¡å‹

```python
# æœ¬åœ°æ¨¡å‹è·¯å¾„
config = get_model_config('/home/user/my_fine_tuned_model', 'small_scale')

# Hugging Face Hub æ¨¡å‹
config = get_model_config('microsoft/DialoGPT-medium', 'quick_test')

# å¸¦è‡ªå®šä¹‰å‚æ•°çš„æœ¬åœ°æ¨¡å‹
from configs.multimodel_config import create_local_model_config
config = create_local_model_config(
    '/path/to/local/model',
    model_layers=4,
    batch_size=8,
    trust_remote_code=True
)
```

## ğŸ› ï¸ é…ç½®ç³»ç»Ÿ

### æŸ¥çœ‹å¯ç”¨æ¨¡å‹

```python
from configs.multimodel_config import print_available_models
print_available_models()
```

### å®éªŒè§„æ¨¡é…ç½®

| è§„æ¨¡          | åºåˆ—é•¿åº¦ | é¢„æµ‹é•¿åº¦ | è®­ç»ƒè½®æ•° | é€‚ç”¨åœºæ™¯   |
| ------------- | -------- | -------- | -------- | ---------- |
| `quick_test`  | 168      | 24       | 2        | å¿«é€ŸéªŒè¯   |
| `small_scale` | 336      | 96       | 5        | å°è§„æ¨¡å®éªŒ |
| `full_scale`  | 720      | 192      | 20       | å®Œæ•´è®­ç»ƒ   |

### è‡ªå®šä¹‰é…ç½®

```python
from types import SimpleNamespace

# å®Œå…¨è‡ªå®šä¹‰é…ç½®
custom_config = SimpleNamespace(
    model_name_or_path='bert-base-uncased',  # æ¨¡å‹è·¯å¾„
    seq_len=480,            # è¾“å…¥åºåˆ—é•¿åº¦
    pred_len=120,           # é¢„æµ‹é•¿åº¦
    patch_size=20,          # Patchå¤§å°
    stride=10,              # æ­¥é•¿
    model_layers=8,         # ä½¿ç”¨å±‚æ•°
    batch_size=16,          # æ‰¹æ¬¡å¤§å°
    learning_rate=2e-5,     # å­¦ä¹ ç‡
    freeze=True,            # å‚æ•°å†»ç»“
    trust_remote_code=False # æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
)
```

## ğŸ§ª æµ‹è¯•æ¨¡å‹

### è¿è¡Œé¢„è®¾æ¨¡å‹æµ‹è¯•

```bash
# æµ‹è¯•é»˜è®¤çš„é¢„è®¾æ¨¡å‹
python test_multimodel.py

# æµ‹è¯•ç‰¹å®šæ¨¡å‹
python test_multimodel.py bert-base-uncased

# æµ‹è¯•æœ¬åœ°æ¨¡å‹
python test_multimodel.py /path/to/local/model
```

### ç¨‹åºåŒ–æµ‹è¯•

```python
from test_multimodel import test_custom_model

# æµ‹è¯•æœ¬åœ°æ¨¡å‹
test_custom_model('/path/to/my/model')

# æµ‹è¯• Hub æ¨¡å‹  
test_custom_model('microsoft/DialoGPT-small')
```

### æµ‹è¯•è¾“å‡ºç¤ºä¾‹

```
ğŸš€ å¼€å§‹å¤šæ¨¡å‹æ—¶é—´åºåˆ—é¢„æµ‹æµ‹è¯•
============================================================
ğŸ”§ ä½¿ç”¨è®¾å¤‡: cuda

ğŸ¯ å¯ç”¨çš„é¢„è®¾æ¨¡å‹:
============================================================

ğŸ“š GPT-2 ç³»åˆ—:
   gpt2            - GPT-2 base (124M å‚æ•°)
   gpt2-medium     - GPT-2 medium (355M å‚æ•°)
   gpt2-large      - GPT-2 large (774M å‚æ•°)

ğŸ“š BERT ç³»åˆ—:
   bert-base       - BERT base uncased (110M å‚æ•°)
   bert-large      - BERT large uncased (340M å‚æ•°)
   distilbert      - DistilBERT base (66M å‚æ•°)
   roberta-base    - RoBERTa base (125M å‚æ•°)

==================================================
æµ‹è¯•æ¨¡å‹: gpt2
==================================================
ğŸ“‹ ä½¿ç”¨é¢„è®¾æ¨¡å‹é…ç½®: gpt2
ğŸ“ æ¨¡å‹æè¿°: GPT-2 base (124M å‚æ•°)
ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: gpt2
âœ… æˆåŠŸåŠ è½½æ¨¡å‹: gpt2
ğŸ“Š æ¨¡å‹ç±»å‹: GPT2Model
ğŸ“ æ£€æµ‹åˆ°æ¨¡å‹éšè—ç»´åº¦: 768 (æ¥è‡ª config.n_embd)
âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ
ğŸ“Š æ¨¡å‹ç»Ÿè®¡:
   - æ¨¡å‹è·¯å¾„: gpt2
   - æ¨¡å‹æè¿°: GPT-2 base (124M å‚æ•°)
   - æ€»å‚æ•°æ•°é‡: 124,439,808
   - å¯è®­ç»ƒå‚æ•°: 1,574,400
   - å‚æ•°å†»ç»“æ¯”ä¾‹: 98.7%
â±ï¸  æ€§èƒ½æŒ‡æ ‡:
   - æ¨¡å‹åˆ›å»ºæ—¶é—´: 2.341s
   - æ¨ç†æ—¶é—´: 0.145s
   - MSE Loss: 0.234567
   - MAE Loss: 0.123456

ğŸ† æœ€ä½³æ¨¡å‹: gpt2 (MSE: 0.234567)
ğŸ“ˆ é¢„æµ‹ç»“æœå¯è§†åŒ–å·²ä¿å­˜åˆ°: predictions_comparison.png
```

## ğŸ‹ï¸ è®­ç»ƒæ¨¡å‹

### æŸ¥çœ‹å¯ç”¨æ¨¡å‹

```bash
# æ˜¾ç¤ºæ‰€æœ‰é¢„è®¾æ¨¡å‹
python train_multimodel.py --list_models
```

### åŸºç¡€è®­ç»ƒ

```bash
# è®­ç»ƒé¢„è®¾æ¨¡å‹
python train_multimodel.py --model gpt2 --experiment quick_test
python train_multimodel.py --model bert-base --experiment small_scale
python train_multimodel.py --model distilbert --experiment full_scale

# è®­ç»ƒæœ¬åœ°æ¨¡å‹
python train_multimodel.py --model /path/to/local/model --experiment small_scale

# è®­ç»ƒ Hub æ¨¡å‹
python train_multimodel.py --model microsoft/DialoGPT-medium --experiment quick_test
```

### é«˜çº§è®­ç»ƒé€‰é¡¹

```bash
# å®Œæ•´å‘½ä»¤ç¤ºä¾‹
python train_multimodel.py \
    --model bert-base-uncased \
    --experiment small_scale \
    --save_dir ./my_checkpoints \
    --device cuda
```

### è®­ç»ƒè¾“å‡ºç¤ºä¾‹

```
ğŸ“‹ ä½¿ç”¨é¢„è®¾æ¨¡å‹é…ç½®: gpt2
ğŸ“ æ¨¡å‹æè¿°: GPT-2 base (124M å‚æ•°)
ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: gpt2
âœ… æˆåŠŸåŠ è½½æ¨¡å‹: gpt2
ğŸ“Š æ¨¡å‹ç±»å‹: GPT2Model
ğŸ“ æ£€æµ‹åˆ°æ¨¡å‹éšè—ç»´åº¦: 768 (æ¥è‡ª config.n_embd)
ğŸ”§ é™åˆ¶æ¨¡å‹å±‚æ•°ä¸º: 2
âœ… æˆåŠŸé™åˆ¶ h å±‚æ•°: 12 -> 2
ğŸ§Š åº”ç”¨å‚æ•°å†»ç»“ç­–ç•¥...
ğŸ“Š å‚æ•°å†»ç»“ç»Ÿè®¡:
   - æ€»å‚æ•°æ•°: 24,515,584
   - å†»ç»“å‚æ•°: 23,592,960
   - å¯è®­ç»ƒå‚æ•°: 922,624
   - å†»ç»“æ¯”ä¾‹: 96.2%

ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹: gpt2
============================================================
ğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨...
   - è®­ç»ƒæ ·æœ¬: 1000
   - éªŒè¯æ ·æœ¬: 200

ğŸ“ˆ Epoch 1/2
----------------------------------------
    Batch 0/250, Loss: 1.234567
   è®­ç»ƒæŸå¤±: 0.876543
   éªŒè¯æŸå¤±: 0.765432
   å­¦ä¹ ç‡: 1.00e-04
   ç”¨æ—¶: 12.34s
   âœ… æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (éªŒè¯æŸå¤±: 0.765432)

ğŸ‰ è®­ç»ƒå®Œæˆ!
   æœ€ä½³éªŒè¯æŸå¤±: 0.543210
```

## ğŸ“Š æ¨¡å‹é…ç½®è¯¦è§£

### æ™ºèƒ½å‚æ•°æ£€æµ‹

æ¡†æ¶ä¼šè‡ªåŠ¨æ£€æµ‹æ¨¡å‹çš„å…³é”®å‚æ•°ï¼š

```python
# è‡ªåŠ¨æ£€æµ‹éšè—ç»´åº¦
hidden_size_attrs = ['hidden_size', 'd_model', 'n_embd', 'dim', 'model_dim']

# è‡ªåŠ¨é™åˆ¶æ¨¡å‹å±‚æ•°
layer_attrs = ['layers', 'layer', 'h', 'encoder.layer', 'transformer.h']
```

### é€šç”¨å†»ç»“ç­–ç•¥

```python
# åªè®­ç»ƒè¿™äº›ç±»å‹çš„å‚æ•°
trainable_patterns = [
    'norm',           # å„ç§å½’ä¸€åŒ–å±‚
    'ln',             # LayerNorm (GPT é£æ ¼)
    'layer_norm',     # LayerNorm (BERT é£æ ¼)
    'rmsnorm',        # RMSNorm (LLaMA é£æ ¼)
    'embed',          # åµŒå…¥å±‚
    'position',       # ä½ç½®ç¼–ç 
]
```

## ğŸ¯ æœ€ä½³å®è·µ

### æ¨¡å‹é€‰æ‹©å»ºè®®

| ä»»åŠ¡ç‰¹ç‚¹       | æ¨èæ¨¡å‹      | é…ç½®å»ºè®®            |
| -------------- | ------------- | ------------------- |
| **å¿«é€ŸåŸå‹**   | `distilbert`  | `quick_test`        |
| **æ ‡å‡†é¢„æµ‹**   | `gpt2`        | `small_scale`       |
| **é«˜ç²¾åº¦éœ€æ±‚** | `bert-base`   | `full_scale`        |
| **èµ„æºå—é™**   | `distilbert`  | å‡å°‘ `batch_size`   |
| **å¤æ‚æ¨¡å¼**   | `gpt2-medium` | å¢åŠ  `model_layers` |

### æœ¬åœ°æ¨¡å‹ä½¿ç”¨

```bash
# 1. å‡†å¤‡æœ¬åœ°æ¨¡å‹ç›®å½•
/path/to/my/model/
â”œâ”€â”€ config.json
â”œâ”€â”€ pytorch_model.bin
â””â”€â”€ tokenizer.json

# 2. åˆ›å»ºé…ç½®
python -c "
from configs.multimodel_config import create_local_model_config
config = create_local_model_config('/path/to/my/model')
print(config.model_name_or_path)
"

# 3. å¼€å§‹è®­ç»ƒ
python train_multimodel.py --model /path/to/my/model
```

### æ€§èƒ½ä¼˜åŒ–

```python
# å‡å°‘æ˜¾å­˜å ç”¨
config.batch_size = 4
config.model_layers = 2
config.freeze = True

# åŠ é€Ÿè®­ç»ƒ
config.num_epochs = 5
config.patience = 2

# æé«˜ç²¾åº¦
config.model_layers = 8
config.seq_len = 720
```

## ğŸ› å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•æ·»åŠ æ–°çš„é¢„è®¾æ¨¡å‹ï¼Ÿ

```python
# åœ¨ configs/multimodel_config.py ä¸­æ·»åŠ 
NEW_MODELS = {
    'my-model': {
        'model_name_or_path': 'my-org/my-model',
        'description': 'æˆ‘çš„è‡ªå®šä¹‰æ¨¡å‹',
        'd_model': 768,
        'model_layers': 6,
        'learning_rate': 1e-4,
        'batch_size': 16,
    }
}

# ç„¶åæ·»åŠ åˆ° PRESET_MODELS
PRESET_MODELS = {
    **GPT2_MODELS,
    **BERT_MODELS,
    **LLM_MODELS,
    **NEW_MODELS  # æ·»åŠ è¿™è¡Œ
}
```

### Q2: æ¨¡å‹åŠ è½½å¤±è´¥

```
âŒ æ¨¡å‹åŠ è½½å¤±è´¥: my-model
é”™è¯¯ä¿¡æ¯: Can't load tokenizer for 'my-model'
```

**è§£å†³æ–¹æ¡ˆ:**
```python
# è®¾ç½® trust_remote_code=True
config = get_model_config('my-model', 'quick_test')
config.trust_remote_code = True

# æˆ–è€…ç›´æ¥åœ¨é¢„è®¾ä¸­é…ç½®
'trust_remote_code': True
```

### Q3: æ˜¾å­˜ä¸è¶³

```python
# æ–¹æ¡ˆ1: å‡å°‘æ‰¹æ¬¡å¤§å°
config.batch_size = 2

# æ–¹æ¡ˆ2: å‡å°‘æ¨¡å‹å±‚æ•°
config.model_layers = 2

# æ–¹æ¡ˆ3: ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
# åœ¨æ¨¡å‹è®­ç»ƒæ—¶è®¾ç½®
torch.utils.checkpoint.checkpoint_sequential = True
```

### Q4: æœ¬åœ°æ¨¡å‹è·¯å¾„é—®é¢˜

```bash
# Windows è·¯å¾„
python train_multimodel.py --model "C:\Users\Name\model"

# Linux/Mac è·¯å¾„  
python train_multimodel.py --model "/home/user/model"

# ç›¸å¯¹è·¯å¾„
python train_multimodel.py --model "./models/my_model"
```

## ğŸ“ˆ æ‰©å±•åŠŸèƒ½

### è‡ªå®šä¹‰æ•°æ®åŠ è½½å™¨

```python
def create_custom_dataloader(self, data_path):
    """åŠ è½½çœŸå®æ•°æ®"""
    # åŠ è½½ä½ çš„æ—¶é—´åºåˆ—æ•°æ®
    data = np.load(data_path)
    # ... æ•°æ®é¢„å¤„ç†
    return dataloader
```

### è‡ªå®šä¹‰æŸå¤±å‡½æ•°

```python
class CustomLoss(nn.Module):
    def forward(self, pred, true):
        mse = nn.MSELoss()(pred, true)
        mae = nn.L1Loss()(pred, true)
        return mse + 0.1 * mae

# åœ¨è®­ç»ƒå™¨ä¸­ä½¿ç”¨
self.criterion = CustomLoss()
```

### é›†æˆå…¶ä»–æ¨¡å‹

```python
# åœ¨é…ç½®ä¸­æ·»åŠ æ–°æ¨¡å‹
'custom-transformer': {
    'model_name_or_path': 'your-org/custom-transformer',
    'description': 'è‡ªå®šä¹‰ Transformer æ¨¡å‹',
    'trust_remote_code': True,
}
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

- **Hugging Face Transformers**: æä¾›ç»Ÿä¸€çš„æ¨¡å‹æ¥å£
- **PyTorch**: æ·±åº¦å­¦ä¹ æ¡†æ¶
- **å„ä¸ªé¢„è®­ç»ƒæ¨¡å‹çš„åŸä½œè€…ä»¬**

---

**å¿«é€Ÿä¸Šæ‰‹å‘½ä»¤:**

```bash
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹
python train_multimodel.py --list_models

# å¿«é€Ÿæµ‹è¯•
python test_multimodel.py

# å¼€å§‹è®­ç»ƒ
python train_multimodel.py --model gpt2 --experiment quick_test
```

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤ Issue æˆ–è”ç³»ç»´æŠ¤è€…ã€‚ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼ ğŸ‰ 