#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æµ‹è¯•å¤šæ¨¡å‹æ—¶é—´åºåˆ—é¢„æµ‹æ¶æ„
ä½¿ç”¨ Hugging Face ç»Ÿä¸€æ¥å£æµ‹è¯•ä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹
"""

import torch
import torch.nn as nn
import numpy as np
from types import SimpleNamespace
import time
import matplotlib.pyplot as plt
from models.GPT4TS import MultiModelTS
from configs.multimodel_config import get_model_config, get_preset_model_list, print_available_models


def create_synthetic_data(batch_size=32, seq_len=336, pred_len=96, num_variables=7):
    """åˆ›å»ºåˆæˆçš„æ—¶é—´åºåˆ—æ•°æ®ç”¨äºæµ‹è¯•
    
    Args:
        batch_size (int): æ‰¹æ¬¡å¤§å°
        seq_len (int): è¾“å…¥åºåˆ—é•¿åº¦
        pred_len (int): é¢„æµ‹é•¿åº¦
        num_variables (int): å˜é‡æ•°é‡
    
    Returns:
        tuple: (è¾“å…¥æ•°æ®, çœŸå®æ ‡ç­¾)
    """
    # åˆ›å»ºå…·æœ‰å­£èŠ‚æ€§å’Œè¶‹åŠ¿çš„åˆæˆæ—¶é—´åºåˆ—
    t = np.linspace(0, 4*np.pi, seq_len + pred_len)
    
    data = []
    for b in range(batch_size):
        series = []
        for v in range(num_variables):
            # ä¸åŒå˜é‡å…·æœ‰ä¸åŒçš„é¢‘ç‡å’Œç›¸ä½
            freq = 0.5 + v * 0.1
            phase = v * np.pi / 4
            noise = np.random.normal(0, 0.1, len(t))
            
            # ç»„åˆæ­£å¼¦æ³¢ã€çº¿æ€§è¶‹åŠ¿å’Œå™ªå£°
            signal = (np.sin(freq * t + phase) + 
                     0.1 * t + 
                     0.3 * np.sin(2 * freq * t) + 
                     noise)
            series.append(signal)
        
        data.append(np.array(series).T)  # (seq_len + pred_len, num_variables)
    
    data = np.array(data)  # (batch_size, seq_len + pred_len, num_variables)
    
    # åˆ†å‰²è¾“å…¥å’Œæ ‡ç­¾
    x = data[:, :seq_len, :]  # (batch_size, seq_len, num_variables)
    y = data[:, seq_len:, :]  # (batch_size, pred_len, num_variables)
    
    return torch.FloatTensor(x), torch.FloatTensor(y)


def test_single_model(model_key_or_path, device, x, y_true, experiment_type='quick_test'):
    """æµ‹è¯•å•ä¸ªæ¨¡å‹çš„æ€§èƒ½
    
    Args:
        model_key_or_path (str): æ¨¡å‹é”®åæˆ–è·¯å¾„
        device (torch.device): è®¾å¤‡
        x (torch.Tensor): è¾“å…¥æ•°æ®
        y_true (torch.Tensor): çœŸå®æ ‡ç­¾
        experiment_type (str): å®éªŒç±»å‹
    
    Returns:
        dict: åŒ…å«é¢„æµ‹ç»“æœå’Œæ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
    """
    print(f"\n{'='*50}")
    print(f"æµ‹è¯•æ¨¡å‹: {model_key_or_path}")
    print(f"{'='*50}")
    
    try:
        # è·å–æ¨¡å‹é…ç½®
        config = get_model_config(model_key_or_path, experiment_type)
        
        # åˆ›å»ºæ¨¡å‹
        start_time = time.time()
        model = MultiModelTS(config, device)
        model_creation_time = time.time() - start_time
        
        # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
        x_device = x.to(device)
        y_true_device = y_true.to(device)
        
        # å‰å‘ä¼ æ’­
        model.eval()
        with torch.no_grad():
            start_time = time.time()
            y_pred = model(x_device, itr=0)
            inference_time = time.time() - start_time
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        mse = nn.MSELoss()(y_pred, y_true_device).item()
        mae = nn.L1Loss()(y_pred, y_true_device).item()
        
        # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"ğŸ“Š æ¨¡å‹ç»Ÿè®¡:")
        print(f"   - æ¨¡å‹è·¯å¾„: {config.model_name_or_path}")
        if hasattr(config, 'description'):
            print(f"   - æ¨¡å‹æè¿°: {config.description}")
        print(f"   - æ€»å‚æ•°æ•°é‡: {total_params:,}")
        print(f"   - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"   - å‚æ•°å†»ç»“æ¯”ä¾‹: {(total_params-trainable_params)/total_params*100:.1f}%")
        print(f"â±ï¸  æ€§èƒ½æŒ‡æ ‡:")
        print(f"   - æ¨¡å‹åˆ›å»ºæ—¶é—´: {model_creation_time:.3f}s")
        print(f"   - æ¨ç†æ—¶é—´: {inference_time:.3f}s")
        print(f"   - MSE Loss: {mse:.6f}")
        print(f"   - MAE Loss: {mae:.6f}")
        
        return {
            'model': model,
            'config': config,
            'prediction': y_pred.cpu(),
            'mse': mse,
            'mae': mae,
            'total_params': total_params,
            'trainable_params': trainable_params,
            'model_creation_time': model_creation_time,
            'inference_time': inference_time,
            'success': True
        }
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ {model_key_or_path} æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e)
        }


def visualize_predictions(results, x, y_true, save_path='predictions_comparison.png'):
    """å¯è§†åŒ–ä¸åŒæ¨¡å‹çš„é¢„æµ‹ç»“æœ
    
    Args:
        results (dict): å„æ¨¡å‹çš„æµ‹è¯•ç»“æœ
        x (torch.Tensor): è¾“å…¥æ•°æ®
        y_true (torch.Tensor): çœŸå®æ ‡ç­¾
        save_path (str): ä¿å­˜è·¯å¾„
    """
    successful_models = {k: v for k, v in results.items() if v.get('success', False)}
    
    if not successful_models:
        print("æ²¡æœ‰æˆåŠŸçš„æ¨¡å‹å¯ä¾›å¯è§†åŒ–")
        return
    
    num_models = len(successful_models)
    fig, axes = plt.subplots(num_models, 1, figsize=(15, 4*num_models))
    
    if num_models == 1:
        axes = [axes]
    
    # åªæ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ªå˜é‡
    sample_idx, var_idx = 0, 0
    
    for idx, (model_key, result) in enumerate(successful_models.items()):
        ax = axes[idx]
        
        # å‡†å¤‡æ•°æ®
        input_seq = x[sample_idx, :, var_idx].numpy()
        true_pred = y_true[sample_idx, :, var_idx].numpy()
        model_pred = result['prediction'][sample_idx, :, var_idx].numpy()
        
        # åˆ›å»ºæ—¶é—´è½´
        input_time = range(len(input_seq))
        pred_time = range(len(input_seq), len(input_seq) + len(true_pred))
        
        # ç»˜åˆ¶
        ax.plot(input_time, input_seq, 'b-', label='å†å²æ•°æ®', linewidth=2)
        ax.plot(pred_time, true_pred, 'g-', label='çœŸå®å€¼', linewidth=2)
        ax.plot(pred_time, model_pred, 'r--', label=f'{model_key} é¢„æµ‹', linewidth=2)
        
        # æ·»åŠ åˆ†ç•Œçº¿
        ax.axvline(x=len(input_seq)-1, color='gray', linestyle=':', alpha=0.7)
        
        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        ax.set_title(f'{model_key} æ¨¡å‹é¢„æµ‹ç»“æœ (MSE: {result["mse"]:.6f})', fontsize=14)
        ax.set_xlabel('æ—¶é—´æ­¥')
        ax.set_ylabel('æ•°å€¼')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“ˆ é¢„æµ‹ç»“æœå¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¤šæ¨¡å‹æ—¶é—´åºåˆ—é¢„æµ‹æµ‹è¯•")
    print("=" * 60)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ˜¾ç¤ºå¯ç”¨çš„é¢„è®¾æ¨¡å‹
    print("\n")
    print_available_models()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print(f"\nğŸ“Š åˆ›å»ºåˆæˆæµ‹è¯•æ•°æ®...")
    x, y_true = create_synthetic_data(batch_size=4, seq_len=168, pred_len=24, num_variables=7)
    print(f"   - è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"   - æ ‡ç­¾å½¢çŠ¶: {y_true.shape}")
    
    # è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨ï¼ˆä»é¢„è®¾æ¨¡å‹ä¸­é€‰æ‹©è¾ƒå°çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼‰
    models_to_test = [
        'gpt2',           # GPT-2 base
        'distilbert',     # DistilBERT (è¾ƒå°çš„BERT)
        'bert-base',      # BERT base
    ]
    
    # å¯é€‰ï¼šå¦‚æœç”¨æˆ·æŒ‡å®šäº†æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œä¹Ÿå¯ä»¥æµ‹è¯•
    # ä¾‹å¦‚: models_to_test.append('/path/to/local/model')
    
    print(f"\nğŸ¯ å°†æµ‹è¯•ä»¥ä¸‹æ¨¡å‹: {', '.join(models_to_test)}")
    
    # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
    results = {}
    experiment_type = 'quick_test'  # ä½¿ç”¨å¿«é€Ÿæµ‹è¯•é…ç½®
    
    for model_key in models_to_test:
        results[model_key] = test_single_model(
            model_key, device, x, y_true, experiment_type
        )
    
    # æ‰“å°æ€»ç»“
    print(f"\n{'='*60}")
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“")
    print(f"{'='*60}")
    
    successful_models = []
    failed_models = []
    
    for model_key, result in results.items():
        if result.get('success', False):
            successful_models.append((model_key, result))
            print(f"âœ… {model_key}: MSE={result['mse']:.6f}, å‚æ•°={result['total_params']:,}")
        else:
            failed_models.append((model_key, result.get('error', 'Unknown error')))
            print(f"âŒ {model_key}: {result.get('error', 'Unknown error')}")
    
    if successful_models:
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = min(successful_models, key=lambda x: x[1]['mse'])
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model[0]} (MSE: {best_model[1]['mse']:.6f})")
        
        # æ€§èƒ½å¯¹æ¯”è¡¨æ ¼
        print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
        print(f"{'æ¨¡å‹':<15} {'MSE':<10} {'å‚æ•°æ•°é‡':<12} {'æ¨ç†æ—¶é—´':<10}")
        print("-" * 50)
        for model_key, result in successful_models:
            print(f"{model_key:<15} {result['mse']:<10.6f} {result['total_params']:<12,} {result['inference_time']:<10.3f}s")
        
        # å¯è§†åŒ–ç»“æœ
        try:
            visualize_predictions(results, x, y_true)
        except Exception as e:
            print(f"âš ï¸  å¯è§†åŒ–å¤±è´¥: {str(e)}")
    
    if failed_models:
        print(f"\nâš ï¸  å¤±è´¥çš„æ¨¡å‹:")
        for model_key, error in failed_models:
            print(f"   {model_key}: {error}")
        print(f"\nğŸ’¡ æç¤º:")
        print(f"   - æŸäº›æ¨¡å‹å¯èƒ½éœ€è¦é¢å¤–çš„ä¾èµ–æˆ–æƒé™")
        print(f"   - å¯ä»¥å°è¯•ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹æˆ–æœ¬åœ°æ¨¡å‹")
        print(f"   - æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆ!")


def test_custom_model(model_path):
    """æµ‹è¯•è‡ªå®šä¹‰æ¨¡å‹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model_path (str): æœ¬åœ°æ¨¡å‹è·¯å¾„æˆ– Hub æ¨¡å‹åç§°
    """
    print(f"ğŸ§ª æµ‹è¯•è‡ªå®šä¹‰æ¨¡å‹: {model_path}")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x, y_true = create_synthetic_data(batch_size=2, seq_len=168, pred_len=24, num_variables=7)
    
    # æµ‹è¯•æ¨¡å‹
    result = test_single_model(model_path, device, x, y_true, 'quick_test')
    
    if result['success']:
        print(f"âœ… è‡ªå®šä¹‰æ¨¡å‹æµ‹è¯•æˆåŠŸ!")
        print(f"   MSE: {result['mse']:.6f}")
        print(f"   æ€»å‚æ•°: {result['total_params']:,}")
    else:
        print(f"âŒ è‡ªå®šä¹‰æ¨¡å‹æµ‹è¯•å¤±è´¥: {result['error']}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        # å¦‚æœæä¾›äº†å‘½ä»¤è¡Œå‚æ•°ï¼Œæµ‹è¯•æŒ‡å®šçš„æ¨¡å‹
        custom_model = sys.argv[1]
        test_custom_model(custom_model)
    else:
        # å¦åˆ™è¿è¡Œæ ‡å‡†æµ‹è¯•
        main() 